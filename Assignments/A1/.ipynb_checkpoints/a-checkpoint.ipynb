{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c25b279",
   "metadata": {},
   "source": [
    "# Q1: Regression Decision Tree Construction\n",
    "\n",
    "### Group Members: Pranav Mehrotra (20CS10085) and Saransh Sharma (20CS30065)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5174c520",
   "metadata": {},
   "source": [
    "#### Import Required Libraries. To install Seaborn type in command pip install seaborn in the terminal. \n",
    "#### To run a cell press ctr + enter and press shift + enter to run a cell and move to next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61d4988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09a9275",
   "metadata": {},
   "source": [
    "#### Read the CSV file in the from of a dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bed8727",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Train_B_Tree.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a509f27a",
   "metadata": {},
   "source": [
    "#### Primary Analysis of the data read. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3712972",
   "metadata": {},
   "source": [
    "#### Check for duplicate data. Duplicate data doesn't help in training and so needs to be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b282f617",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cement</th>\n",
       "      <th>slag</th>\n",
       "      <th>flyash</th>\n",
       "      <th>water</th>\n",
       "      <th>superplasticizer</th>\n",
       "      <th>coarseaggregate</th>\n",
       "      <th>fineaggregate</th>\n",
       "      <th>age</th>\n",
       "      <th>csMPa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>425.0</td>\n",
       "      <td>106.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>153.5</td>\n",
       "      <td>16.5</td>\n",
       "      <td>852.1</td>\n",
       "      <td>887.1</td>\n",
       "      <td>3</td>\n",
       "      <td>33.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>425.0</td>\n",
       "      <td>106.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>153.5</td>\n",
       "      <td>16.5</td>\n",
       "      <td>852.1</td>\n",
       "      <td>887.1</td>\n",
       "      <td>3</td>\n",
       "      <td>33.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>362.6</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>164.9</td>\n",
       "      <td>11.6</td>\n",
       "      <td>944.7</td>\n",
       "      <td>755.8</td>\n",
       "      <td>3</td>\n",
       "      <td>35.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>362.6</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>164.9</td>\n",
       "      <td>11.6</td>\n",
       "      <td>944.7</td>\n",
       "      <td>755.8</td>\n",
       "      <td>3</td>\n",
       "      <td>35.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>362.6</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>164.9</td>\n",
       "      <td>11.6</td>\n",
       "      <td>944.7</td>\n",
       "      <td>755.8</td>\n",
       "      <td>3</td>\n",
       "      <td>35.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>425.0</td>\n",
       "      <td>106.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>153.5</td>\n",
       "      <td>16.5</td>\n",
       "      <td>852.1</td>\n",
       "      <td>887.1</td>\n",
       "      <td>7</td>\n",
       "      <td>49.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>425.0</td>\n",
       "      <td>106.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>153.5</td>\n",
       "      <td>16.5</td>\n",
       "      <td>852.1</td>\n",
       "      <td>887.1</td>\n",
       "      <td>7</td>\n",
       "      <td>49.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>362.6</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>164.9</td>\n",
       "      <td>11.6</td>\n",
       "      <td>944.7</td>\n",
       "      <td>755.8</td>\n",
       "      <td>7</td>\n",
       "      <td>55.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>362.6</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>164.9</td>\n",
       "      <td>11.6</td>\n",
       "      <td>944.7</td>\n",
       "      <td>755.8</td>\n",
       "      <td>7</td>\n",
       "      <td>55.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>425.0</td>\n",
       "      <td>106.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>153.5</td>\n",
       "      <td>16.5</td>\n",
       "      <td>852.1</td>\n",
       "      <td>887.1</td>\n",
       "      <td>28</td>\n",
       "      <td>60.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>425.0</td>\n",
       "      <td>106.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>153.5</td>\n",
       "      <td>16.5</td>\n",
       "      <td>852.1</td>\n",
       "      <td>887.1</td>\n",
       "      <td>28</td>\n",
       "      <td>60.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>362.6</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>164.9</td>\n",
       "      <td>11.6</td>\n",
       "      <td>944.7</td>\n",
       "      <td>755.8</td>\n",
       "      <td>28</td>\n",
       "      <td>71.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>362.6</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>164.9</td>\n",
       "      <td>11.6</td>\n",
       "      <td>944.7</td>\n",
       "      <td>755.8</td>\n",
       "      <td>28</td>\n",
       "      <td>71.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>362.6</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>164.9</td>\n",
       "      <td>11.6</td>\n",
       "      <td>944.7</td>\n",
       "      <td>755.8</td>\n",
       "      <td>28</td>\n",
       "      <td>71.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>425.0</td>\n",
       "      <td>106.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>153.5</td>\n",
       "      <td>16.5</td>\n",
       "      <td>852.1</td>\n",
       "      <td>887.1</td>\n",
       "      <td>56</td>\n",
       "      <td>64.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>425.0</td>\n",
       "      <td>106.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>153.5</td>\n",
       "      <td>16.5</td>\n",
       "      <td>852.1</td>\n",
       "      <td>887.1</td>\n",
       "      <td>56</td>\n",
       "      <td>64.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>362.6</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>164.9</td>\n",
       "      <td>11.6</td>\n",
       "      <td>944.7</td>\n",
       "      <td>755.8</td>\n",
       "      <td>56</td>\n",
       "      <td>77.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>362.6</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>164.9</td>\n",
       "      <td>11.6</td>\n",
       "      <td>944.7</td>\n",
       "      <td>755.8</td>\n",
       "      <td>56</td>\n",
       "      <td>77.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>362.6</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>164.9</td>\n",
       "      <td>11.6</td>\n",
       "      <td>944.7</td>\n",
       "      <td>755.8</td>\n",
       "      <td>56</td>\n",
       "      <td>77.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>425.0</td>\n",
       "      <td>106.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>153.5</td>\n",
       "      <td>16.5</td>\n",
       "      <td>852.1</td>\n",
       "      <td>887.1</td>\n",
       "      <td>91</td>\n",
       "      <td>65.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>425.0</td>\n",
       "      <td>106.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>153.5</td>\n",
       "      <td>16.5</td>\n",
       "      <td>852.1</td>\n",
       "      <td>887.1</td>\n",
       "      <td>91</td>\n",
       "      <td>65.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>362.6</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>164.9</td>\n",
       "      <td>11.6</td>\n",
       "      <td>944.7</td>\n",
       "      <td>755.8</td>\n",
       "      <td>91</td>\n",
       "      <td>79.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>362.6</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>164.9</td>\n",
       "      <td>11.6</td>\n",
       "      <td>944.7</td>\n",
       "      <td>755.8</td>\n",
       "      <td>91</td>\n",
       "      <td>79.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>362.6</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>164.9</td>\n",
       "      <td>11.6</td>\n",
       "      <td>944.7</td>\n",
       "      <td>755.8</td>\n",
       "      <td>91</td>\n",
       "      <td>79.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>252.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1111.0</td>\n",
       "      <td>784.0</td>\n",
       "      <td>28</td>\n",
       "      <td>19.69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     cement   slag  flyash  water  superplasticizer  coarseaggregate  \\\n",
       "77    425.0  106.3     0.0  153.5              16.5            852.1   \n",
       "80    425.0  106.3     0.0  153.5              16.5            852.1   \n",
       "86    362.6  189.0     0.0  164.9              11.6            944.7   \n",
       "88    362.6  189.0     0.0  164.9              11.6            944.7   \n",
       "91    362.6  189.0     0.0  164.9              11.6            944.7   \n",
       "100   425.0  106.3     0.0  153.5              16.5            852.1   \n",
       "103   425.0  106.3     0.0  153.5              16.5            852.1   \n",
       "109   362.6  189.0     0.0  164.9              11.6            944.7   \n",
       "111   362.6  189.0     0.0  164.9              11.6            944.7   \n",
       "123   425.0  106.3     0.0  153.5              16.5            852.1   \n",
       "126   425.0  106.3     0.0  153.5              16.5            852.1   \n",
       "132   362.6  189.0     0.0  164.9              11.6            944.7   \n",
       "134   362.6  189.0     0.0  164.9              11.6            944.7   \n",
       "137   362.6  189.0     0.0  164.9              11.6            944.7   \n",
       "146   425.0  106.3     0.0  153.5              16.5            852.1   \n",
       "149   425.0  106.3     0.0  153.5              16.5            852.1   \n",
       "155   362.6  189.0     0.0  164.9              11.6            944.7   \n",
       "157   362.6  189.0     0.0  164.9              11.6            944.7   \n",
       "160   362.6  189.0     0.0  164.9              11.6            944.7   \n",
       "169   425.0  106.3     0.0  153.5              16.5            852.1   \n",
       "172   425.0  106.3     0.0  153.5              16.5            852.1   \n",
       "177   362.6  189.0     0.0  164.9              11.6            944.7   \n",
       "179   362.6  189.0     0.0  164.9              11.6            944.7   \n",
       "182   362.6  189.0     0.0  164.9              11.6            944.7   \n",
       "809   252.0    0.0     0.0  185.0               0.0           1111.0   \n",
       "\n",
       "     fineaggregate  age  csMPa  \n",
       "77           887.1    3  33.40  \n",
       "80           887.1    3  33.40  \n",
       "86           755.8    3  35.30  \n",
       "88           755.8    3  35.30  \n",
       "91           755.8    3  35.30  \n",
       "100          887.1    7  49.20  \n",
       "103          887.1    7  49.20  \n",
       "109          755.8    7  55.90  \n",
       "111          755.8    7  55.90  \n",
       "123          887.1   28  60.29  \n",
       "126          887.1   28  60.29  \n",
       "132          755.8   28  71.30  \n",
       "134          755.8   28  71.30  \n",
       "137          755.8   28  71.30  \n",
       "146          887.1   56  64.30  \n",
       "149          887.1   56  64.30  \n",
       "155          755.8   56  77.30  \n",
       "157          755.8   56  77.30  \n",
       "160          755.8   56  77.30  \n",
       "169          887.1   91  65.20  \n",
       "172          887.1   91  65.20  \n",
       "177          755.8   91  79.30  \n",
       "179          755.8   91  79.30  \n",
       "182          755.8   91  79.30  \n",
       "809          784.0   28  19.69  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.duplicated()==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30583f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6407638c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1005, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e18886",
   "metadata": {},
   "source": [
    "#### Dataframe Data contains the data read from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "500b8b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cement</th>\n",
       "      <th>slag</th>\n",
       "      <th>flyash</th>\n",
       "      <th>water</th>\n",
       "      <th>superplasticizer</th>\n",
       "      <th>coarseaggregate</th>\n",
       "      <th>fineaggregate</th>\n",
       "      <th>age</th>\n",
       "      <th>csMPa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>79.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>61.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "      <td>40.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "      <td>41.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "      <td>44.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cement   slag  flyash  water  superplasticizer  coarseaggregate  \\\n",
       "0   540.0    0.0     0.0  162.0               2.5           1040.0   \n",
       "1   540.0    0.0     0.0  162.0               2.5           1055.0   \n",
       "2   332.5  142.5     0.0  228.0               0.0            932.0   \n",
       "3   332.5  142.5     0.0  228.0               0.0            932.0   \n",
       "4   198.6  132.4     0.0  192.0               0.0            978.4   \n",
       "\n",
       "   fineaggregate  age  csMPa  \n",
       "0          676.0   28  79.99  \n",
       "1          676.0   28  61.89  \n",
       "2          594.0  270  40.27  \n",
       "3          594.0  365  41.05  \n",
       "4          825.5  360  44.30  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2504f3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1005, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dff4450",
   "metadata": {},
   "source": [
    "#### The model is basically a tree containing nodes and edges. There exist two types of nodes in the tree. Leaf nodes and decision nodes. Leaf nodes are the nodes which would be helpful in case of predicting (outputting the final value) while decision nodes will represent set of conditions that would help us to make a decision about the predicted value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a0d8589",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, attribute=None, threshold=None, child_left=None, child_right=None, variance_red=None, leaf_value=None):\n",
    "        \n",
    "        # data members corresponding to decision nodes\n",
    "        self.attribute = attribute\n",
    "        self.threshold = threshold\n",
    "        self.child_left = child_left\n",
    "        self.child_right = child_right\n",
    "        self.variance_red = variance_red\n",
    "        \n",
    "        #data member corresponding to a leaf node\n",
    "        self.leaf_value = leaf_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a69979",
   "metadata": {},
   "source": [
    "##### Kindly note: We have used the same defination of node for both the types of node. A decision node would have leaf_value = None while a leaf_node would have a numerical leaf_value. This difference would help us to differentiate between a leaf node and a decision node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3efebe",
   "metadata": {},
   "source": [
    "#### Class defination of a regression tree which will encapsulate all the functions and operation needed to construct a regression tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9023ddb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class RegressionTree():\n",
    "    def __init__(self, minimum_samples=2, max_depth=2): #constructor that will take two parameters\n",
    " \n",
    "        self.root = None\n",
    "        self.minimum_samples = minimum_samples #min number of samples that should be available for further splitting\n",
    "        self.max_depth = max_depth #max- depth the tree is allowed to grow\n",
    "        #these two parameters act as stopping conditions for the tree\n",
    "        \n",
    "    def variance_reduction(self, parent, left_branch, right_branch): #to find the reduction in variance\n",
    "        \n",
    "        fraction_left = len(left_branch) / len(parent) #fraction of original data in the left branch\n",
    "        fraction_right = len(right_branch) / len(parent) #fraction of original data in right branch\n",
    "        reduction_variance = np.var(parent) - (fraction_left * np.var(left_branch) + fraction_right * np.var(right_branch))\n",
    "        #variance reduction is defined as variance of original data - weighted sum of variance of branches\n",
    "        return reduction_variance\n",
    "    \n",
    "    def split_left_right(self, dataset, index, threshold): #to split the data in two branches depending upon attribute denoted by index and threshold\n",
    "        \n",
    "        left_dataset = np.array([x for x in dataset if x[index]<=threshold]) #left dataset contains all datapoints whose value of the specified attribute is less than or equal to threshold\n",
    "        right_dataset = np.array([x for x in dataset if x[index]>threshold]) #right dataset contains all datapoints whose value of the specified attribute is more than threshold\n",
    "        return left_dataset, right_dataset #return the two partitions\n",
    "    \n",
    "    def cal_leaf_node(self, y):#to calculate the value of a leaf node simple calculate mean of all the datapoints's y value at that node \n",
    "        \n",
    "        leaf_val = np.mean(y)\n",
    "        return leaf_val\n",
    "                \n",
    "    def get_best_feature(self, dataset, number_datapoints, number_attributes): # to get the feature and threshold with maximum variance reduction\n",
    "        \n",
    "        #initialise best_feature dictionary\n",
    "        best_feature = {}\n",
    "        best_feature[\"attribute\"] = None\n",
    "        best_feature[\"threshold\"] = 0\n",
    "        best_feature[\"dataset_left\"] = None\n",
    "        best_feature[\"dataset_right\"] = None\n",
    "        best_feature[\"variance_reduced\"] = 0\n",
    "        \n",
    "        maximum_variance_reduction = -float(\"inf\") #initialise the maximum variance reduction varaiable which will be sed to keep track of current maximum\n",
    "        \n",
    "        for features in range(number_attributes): #iterate over all features\n",
    "            values = dataset[:, features] #extract the feature column\n",
    "            unique_sorted_values = np.unique(values) #find sorted and unique values\n",
    "            #possible threshold would be decided by taking mean of adjacent entries\n",
    "            threshold_array = np.array([(unique_sorted_values[i]+unique_sorted_values[i+1])/2 for i in range(0,len(unique_sorted_values)-1)])\n",
    "\n",
    "            for threshold in threshold_array: #iterate over all possible threshold values\n",
    "                dataset_left, dataset_right = self.split_left_right(dataset, features, threshold) #split the data according to the feature and threshold\n",
    "                \n",
    "                if len(dataset_left)>0 and len(dataset_right)>0: #if two partitions are created\n",
    "                    \n",
    "                    dataset_y, dataset_left_y, dataset_right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]#extract target variable columns\n",
    "                    \n",
    "                    variance_reduced = self.variance_reduction(dataset_y, dataset_left_y, dataset_right_y)#calculate the reduction in variance caused by this split\n",
    "                    if variance_reduced > maximum_variance_reduction:#if the variance reduction caused is more than the current maxima\n",
    "                        #update the feature dictionary and store all relevant details\n",
    "                        best_feature[\"attribute\"] = features\n",
    "                        best_feature[\"threshold\"] = threshold\n",
    "                        best_feature[\"dataset_left\"] = dataset_left\n",
    "                        best_feature[\"dataset_right\"] = dataset_right\n",
    "                        best_feature[\"variance_reduced\"] = variance_reduced\n",
    "                        maximum_variance_reduction = variance_reduced # update the current maxima and continue iterating over all possible combinations \n",
    "                        \n",
    "        return best_feature # return the maximum variance reducing feature dictionary\n",
    "    \n",
    "    def construct_tree(self, dataset, current_depth=0): #function to construct tree\n",
    "       \n",
    "        X, y = dataset[:,:-1], dataset[:,-1] #extract feature matrix and target variable vector from dataset\n",
    "        number_datapoints, number_attributes = np.shape(X) \n",
    "        current_best_feature = {} #to keep a track of the best splitting attribute for current node \n",
    "        \n",
    "        if number_datapoints >= self.minimum_samples and current_depth <= self.max_depth: #if the stopping conditions are not yet reached\n",
    "            current_best_feature = self.get_best_feature(dataset, number_datapoints, number_attributes) #get the best splitting attribute for the node\n",
    "            if current_best_feature[\"variance_reduced\"]>0: #if the variance reduction is positive that is the data has been splitted in 2 fractions \n",
    "                subtree_left = self.construct_tree(current_best_feature[\"dataset_left\"], current_depth+1) #call construct tree recursively for left subtree\n",
    "                subtree_right = self.construct_tree(current_best_feature[\"dataset_right\"], current_depth+1)#call construct tree recursively for rigjt subtree\n",
    "                return Node(current_best_feature[\"attribute\"], current_best_feature[\"threshold\"],subtree_left, subtree_right, current_best_feature[\"variance_reduced\"])\n",
    "                #return a node with left subtree as left child and right subtree as right child\n",
    "        \n",
    "        #in case the depth is exhausted or we are left with datapoint less than minimum_samples at a node we make that node a leaf node\n",
    "        leaf_value = self.cal_leaf_node(y)#calculate the laef value \n",
    "        return Node(leaf_value = leaf_value)#return the leaf node\n",
    "    \n",
    "    \n",
    "    def print_decision_tree(self,columns,decision_tree=None,indent=\" \"):\n",
    "        \n",
    "        if not decision_tree:#if the tree isn't passed\n",
    "            decision_tree = self.root#the set tree to the root of the class\n",
    "\n",
    "        if decision_tree.leaf_value is not None: #if decision_tree points to a leaf node simply print the value\n",
    "            print(\"Leaf: \",round(decision_tree.leaf_value,3))\n",
    "\n",
    "        else:#if decision tree points to a decision node\n",
    "            #print the node splitting details\n",
    "            print(columns[decision_tree.attribute], \"==>\", round(decision_tree.threshold,3), \"(\", round(decision_tree.variance_red,3),\")\")\n",
    "            \n",
    "            #print the left subtree by recursive calling the function and indentation increasing at every depth\n",
    "            print(\"%sLeft: \" % (indent), end=\"\")\n",
    "            self.print_decision_tree(columns, decision_tree.child_left, indent+indent)\n",
    "            \n",
    "            #print the right subtree by recursive calling the function and indentation increasing at every depth\n",
    "            print(\"%sRight: \" % (indent), end=\"\")\n",
    "            self.print_decision_tree(columns, decision_tree.child_right, indent+indent)\n",
    "    \n",
    "    def fit_model(self, X, y): #train a model to fit X and y\n",
    "        \n",
    "        dataset = np.concatenate((X, y), axis=1)#concatenate X and y to create the dataset\n",
    "        self.root = self.construct_tree(dataset)#train the tree and store the final returned node in root\n",
    "        \n",
    "    def predict(self, data, decision_tree=None):#to predict target variable for a datapoint x\n",
    "        \n",
    "        #basic algo is to traverse the graph depending upon splitting feature and threshold values\n",
    "        \n",
    "        if not decision_tree:#if decision tree is not explicitly passed set decision tree to root\n",
    "            decision_tree = self.root\n",
    "            \n",
    "        if decision_tree.leaf_value!=None: #if you have reached a leaf node simply return the value of the leaf\n",
    "            return decision_tree.leaf_value\n",
    "        \n",
    "        attribute_value = data[decision_tree.attribute]#else extract the value at splitting attrribute column in x  \n",
    "        if attribute_value <= decision_tree.threshold: # check if the value is less than or equal to threshold\n",
    "            return self.predict(data, decision_tree.child_left)#traverse to the left subtree \n",
    "        else:\n",
    "            return self.predict(data, decision_tree.child_right)#else traverse to the right subtree\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2095ecca",
   "metadata": {},
   "source": [
    "#### Error function that will help us in pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93409706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_error(y_pred, y_actual, n): # to calculate root mean square error of the predictions\n",
    "    \n",
    "    sum=0\n",
    "    for i in range(n): #iterate over all n datapoints\n",
    "        sum = sum+(y_pred[i]-y_actual[i])**2 #add to sum the square of the difference between prediction and actual label\n",
    "    \n",
    "    sum = sum/n #take mean of the sum\n",
    "    sum = np.sqrt(sum) #take square root of the error\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae3a11c",
   "metadata": {},
   "source": [
    "#### To select the maximum efficient data split we randomly split the data in 10 sample with 70-30 split and select the distribution that gives us minimum error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2dd5a46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_error=float(\"inf\") #initialise the minimum error\n",
    "\n",
    "for i in range(10): #repeat for 10 splits\n",
    "    d = data.sample(frac = 1,random_state=42) #returns a randomly jumbles data\n",
    "    \n",
    "    div = int(0.7 * d.shape[0])#calculate 70 percent of the number of input datapoints\n",
    "    d_train, d_test = d.iloc[:div,:], d.iloc[div:,:]#split the data into test and train\n",
    "    \n",
    "    d_train_x = d_train.iloc[:,:-1].values#set training data featutre matrix\n",
    "    d_train_y = d_train.iloc[:,-1].values.reshape(-1,1)#set training data output label\n",
    "    d_test_x = d_test.iloc[:,:-1].values#set test data feature matrix\n",
    "    d_test_y = d_test.iloc[:,-1].values.reshape(-1,1)#set test data output label\n",
    "    \n",
    "    regress_tree = RegressionTree(minimum_samples=3, max_depth=15)#construct a regression tree of depth 20(arbitarily large to allow best tree depending upon training set)\n",
    "    regress_tree.fit_model(d_train_x,d_train_y)\n",
    "    y_pred_train = [regress_tree.predict(x,regress_tree.root)  for x in d_train_x]#construct the predicted output variable vector\n",
    "    \n",
    "    if mean_error(y_pred_train,d_train_y,d_train_x.shape[0])<min_error: #if the error of this tree is less than the current minima\n",
    "        min_error = mean_error(y_pred_train,d_train_y,d_train_x.shape[0]) #update current minima\n",
    "        dataset_train = d_train#save the current training dataset\n",
    "        dataset_test = d_test#save the current test set\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e76e8a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cement', 'slag', 'flyash', 'water', 'superplasticizer',\n",
       "       'coarseaggregate', 'fineaggregate', 'age'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = data.iloc[:,:-1].columns #extract the columns of the training data\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d35be2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_x = dataset_train.iloc[:,:-1].values #extract training data feature matrix after best splitting found\n",
    "data_train_y = dataset_train.iloc[:,-1].values.reshape(-1,1) #extract training data target label vector after best splitting found\n",
    "data_test_x = dataset_test.iloc[:,:-1].values #extract test data feature matrix after best splitting found\n",
    "data_test_y = dataset_test.iloc[:,-1].values.reshape(-1,1) #extract test data target label vector after best splitting found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7441a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [] #to store training errors\n",
    "test = [] #to store test error\n",
    "\n",
    "for i in range(1,20):\n",
    "    regress_tree = RegressionTree(minimum_samples=3, max_depth=i)\n",
    "    regress_tree.fit_model(data_train_x,data_train_y)#train a tree of heights 3 to 20\n",
    "    \n",
    "    y_pred_train = [regress_tree.predict(x,regress_tree.root) for x in data_train_x] #calculate training error\n",
    "    train.append(mean_error(y_pred_train,data_train_y,data_train_x.shape[0]))\n",
    "    \n",
    "    y_pred_test = [regress_tree.predict(x,regress_tree.root) for x in data_test_x] #calculate test error\n",
    "    test.append(mean_error(y_pred_test,data_test_y,data_test_x.shape[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0d745b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,10))\n",
    "x = [i for i in range(1,20)] # plot how test and training error vary with depth of the tree\n",
    "plt.plot(x,train);\n",
    "plt.plot(x,test);\n",
    "plt.xlabel(\"Depth\");\n",
    "plt.ylabel(\"Mean Squared Error\");\n",
    "plt.title(\"Error vs Depth\");\n",
    "plt.legend(['Train','Test']);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4594fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,10))\n",
    "x = [i for i in range(1,20)] # plot how test and training error vary with depth of the tree\n",
    "plt.plot(x,train);\n",
    "plt.plot(x,test);\n",
    "plt.xlabel(\"Depth\");\n",
    "plt.ylabel(\"Mean Squared Error\");\n",
    "plt.title(\"Error vs Depth\");\n",
    "plt.legend(['Train','Test']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb4822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test\n",
    "test.index(min(test))\n",
    "test[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13086833",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4162ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,10))\n",
    "x = [i for i in range(9,17)] # plot how test and training error vary with depth of the tree\n",
    "plt.plot(x,test[8:16]);\n",
    "plt.xlabel(\"Depth\");\n",
    "plt.ylabel(\"Mean Squared Error\");\n",
    "plt.title(\"Error vs Depth\");\n",
    "plt.legend(['Train','Test']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d2f0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "min(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7636cc4c",
   "metadata": {},
   "source": [
    "#### We can clearly see the optimal depth of the tree should be around 9 but our present tree has depth 20 which leads to overfitting. The train error has reduced significantly but the tree fails to generalize well on unseen data. Thus, Post-pruning is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa6529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "regress_tree = RegressionTree(minimum_samples=3, max_depth=20)\n",
    "regress_tree.fit_model(data_train_x,data_train_y)#train a tree of heights 20\n",
    "        \n",
    "y_original = [regress_tree.predict(x,regress_tree.root) for x in data_test_x] #calculate test error\n",
    "mean_error(y_original,data_test_y,data_test_x.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49970228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "#copy.copy() makes a dependent copy wherein change in one is reflected in the other copy\n",
    "#copy.deepcopy() makes an independent copy, wherein change in one is not reflected in the other copy\n",
    "def post_pruning(decision_tree,dataset,regress_tree,error=test[-1]):\n",
    "        \n",
    "        X = dataset[:,:-1]\n",
    "        y = dataset[:,-1]\n",
    "        root = regress_tree.root # root will keep a track of the original tree to be pruned\n",
    "        tree1 = copy.copy(root) #tree1 will be a shallow copy of decision tree so that change in one updates the other two\n",
    "        \n",
    "        if tree1.leaf_value is None: #if the node is a decision node\n",
    "            tree1.leaf_value = regress_tree.cal_leaf_node(y)#assign the corresponding leaf value\n",
    "            y_pred = [regress_tree.predict(x,tree1) for x in X]#make predictions on the new tree\n",
    "            \n",
    "            #base condition\n",
    "            if (mean_error(y_pred,y,X.shape[0])) < error:#if the tree is succesful in reducing the error\n",
    "                return tree1#return the root which now has the particular node converted to leaf node\n",
    "            \n",
    "            else:\n",
    "                tree1.leaf_value=None \n",
    "            #recursive defination\n",
    "            else: #in case truncating the branch doesn't help\n",
    "                \n",
    "                #left = [row for row in dataset if row[tree1.attribute]<=tree1.threshold]#access the left subtree \n",
    "                #right = [row for row in dataset if row[tree1.attribute]>tree1.threshold]#access the right subtree\n",
    "                left,right = regress_tree.split_left_right(dataset,tree1.attribute,tree1.threshold)\n",
    "                if tree1.child_left.leaf_value is None:\n",
    "                    left = post_pruning(tree1.child_left,left,regress_tree,error)#prune the left subtree recursively\n",
    "                if tree1.child_right.leaf_value is None:\n",
    "                    right = post_pruning(tree1.child_right,right,regress_tree,error)#prune the right subtree recursively\n",
    "                return Node(root.attribute, root.threshold,left,right,root.variance_red)#create a node with the pruned left subtree and pruned right subtree as left child and right child respectively "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16816bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ed19b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = copy.deepcopy(regress_tree.root)\n",
    "X = dataset_test.iloc[:,:-1].values\n",
    "y = dataset_test.iloc[:,-1].values.reshape(-1,1)\n",
    "dataset = np.concatenate((X, y), axis=1)\n",
    "\n",
    "pruned = post_pruning(tree,dataset,regress_tree)\n",
    "\n",
    "\n",
    "print(\"Error before pruning: \",mean_error(y_original,data_test_y,data_test_x.shape[0]))\n",
    "\n",
    "y_pred_test = [regress_tree.predict(x,pruned) for x in data_test_x] \n",
    "print(\"Error after pruning: \",mean_error(y_pred_test,data_test_y,data_test_x.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd40ee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "regress_tree.print_decision_tree(columns,pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f3a9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,10))\n",
    "x = [i for i in range(3,21)]\n",
    "plt.plot(x,train);\n",
    "plt.plot(x,test);\n",
    "plt.plot(np.full((17,1),mean_error(y_pred_test,data_test_y,data_test_x.shape[0])),np.arange(17))\n",
    "plt.plot(np.full((17,1),mean_error(y_original,data_test_y,data_test_x.shape[0])),np.arange(17))\n",
    "plt.xlabel(\"Depth\");\n",
    "plt.xlabel(\"Depth\");\n",
    "plt.ylabel(\"Mean Squared Error\");\n",
    "plt.title(\"Error vs Depth\");\n",
    "plt.legend(['Train','Test']);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e7d965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c704670",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ee1f9c3c93c55bb4af787bd902f8445c3aef2fd2438d66b6fa4059ca904d1856"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
